<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<!-- Open Sans font -->
<link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:200,200i,300,300i,400,400i,600,600i,700,700i,900,900i|Open+Sans:300,300i,400,400i,600,600i,700,700i,800,800i" id="u-theme-google-font" rel="stylesheet"/>
<!-- Playfair Display font-->
<link href="https://fonts.googleapis.com/css?family=Playfair+Display:400,400i,500,500i,600,600i,700,700i,800,800i,900,900i" id="u-page-google-font" rel="stylesheet"/>
<!-- CSS -->
<link href="https://digital-justice.com/all.css" rel="stylesheet" type="text/css"/>
<!-- Meta tags -->
<!-- Primary Meta Tags -->
<title>Фейсбук се извини за класифицирането на чернокожите мъже като примати</title>
<meta content="Фейсбук се извини за класифицирането на чернокожите мъже като примати" name="title"/>
<meta content="Човек никога не може да гарантира, че ИИ не доставя някакви ужасни странични ефекти, но историята на расовите злоупотреби на Facebook прави вероятно тази грешка да не е последната им." name="description"/>
<!-- FontAwesome -->
<link href="https://digital-justice.com/icons/fontawesome-free-5.13.0-web/css/all.css" rel="stylesheet" type="text/css"/>
<!-- Open Graph / Facebook -->
<meta content="website" property="og:type"/>
<meta content="https://digital-justice.com/" property="og:url"/>
<meta content="Фейсбук се извини за класифицирането на чернокожите мъже като примати" property="og:title"/>
<meta content="Човек никога не може да гарантира, че ИИ не доставя някакви ужасни странични ефекти, но историята на расовите злоупотреби на Facebook прави вероятно тази грешка да не е последната им." property="og:description"/>
<meta content="https://digital-justice.com/images/data-sovereignty-nologo.png" property="og:image"/>
<!-- Twitter -->
<meta content="summary_large_image" property="twitter:card"/>
<meta content="https://digital-justice.com/" property="twitter:url"/>
<meta content="Фейсбук се извини за класифицирането на чернокожите мъже като примати" property="twitter:title"/>
<meta content="Човек никога не може да гарантира, че ИИ не доставя някакви ужасни странични ефекти, но историята на расовите злоупотреби на Facebook прави вероятно тази грешка да не е последната им." property="twitter:description"/>
<meta content="https://digital-justice.com/images/data-sovereignty-nologo.png" property="twitter:image"/>
<!-- Favicon -->
<!-- Favicon -->
<link href="https://digital-justice.com/icons/apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180"/>
<link href="https://digital-justice.com/icons/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png"/>
<link href="https://digital-justice.com/icons/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png"/>
<link href="https://digital-justice.com/icons/site.webmanifest" rel="manifest"/>
</head>
<body>
<header>
<nav>
<a href="../index.html"><i class="fas fa-home"></i> Начало</a>
<a href="../rights.html">Цифрови права</a>
<a href="index.html">Статии</a>
<div id="language-picker"></div>
<a class="hamburger" href="javascript:void(0)" onclick="openHamburger()">
<i class="fa fa-bars"></i>
</a>
</nav>
<script src="https://digital-justice.com/js/open-hamburger.js"></script>
</header>
<section class="simple-orange">
<article class="essay-title">
<h1>Фейсбук се извини за класифицирането на чернокожите мъже като примати</h1>
<p>Това е автентична статия, написана от <a href="https://matrix.to/#/@bramvdnheuvel:nltrix.net">BramvdnHeuvel</a>.</p>
<p>Очаквано време за четене: <img class="icon" src="https://digital-justice.com/images/clock.svg"/> 2 мин.</p>
</article>
</section>
<section class="simple-gray flex-container">
<article class="blog">
<h1>Фейсбук се извини за класифицирането на чернокожите мъже като примати</h1>
<p>Изкуствените интелекти могат да имат расови пристрастия. <a href="https://www.nytimes.com/2019/12/19/technology/facial-recognition-bias.html" target="_blank">Кавказките лица са от 10 до 100 пъти по-склонни да бъдат правилно разпознати и идентифицирани</a>, отколкото афроамериканските и азиатските, което вече е довело до <a href="https://www.nytimes.com/2020/06/24/technology/facial-recognition-arrest.html" target="_blank">неправомерни обвинения, арести и дори лишаване от свобода</a>, защото изкуственият интелект не е могъл правилно да съпостави лице, което не е бяло.</p>
<p>Много ученици са изпитали това от първа ръка по време на пандемията. Много компании като <a href="https://proctorio.com/" target="_blank">Proctorio</a> са постигнали големи успехи в университетите и училищата, където преподавателите инструктират учениците си да инсталират софтуер, който проверява дали правят теста честно. По-вероятно е софтуерът да обвини цветнокожите в измама, защото <a href="https://micky.com.au/proctorio-test-software-fails-to-detect-people-of-color/" target="_blank">софтуерът не може да разпознае лицата им</a>. Доказано е, че подобен софтуер за провеждане на тестове <a href="https://www.technologyreview.com/2020/08/07/1006132/software-algorithms-proctoring-online-tests-ai-ethics/" target="_blank">подсилва превъзходството на бялата раса, сексизма, ableзма и трансфобията</a>.</p>
<h2>Грешка на Facebook</h2>
<p>Извинението на Facebook се отнася до чернокожи мъже в пререкания с бели цивилни и полицаи, <a href="https://www.nytimes.com/2021/09/03/technology/facebook-ai-race-primates.html" target="_blank">според The New York Times</a>. Изкуствен интелект за разпознаване на изображения класифицира клиповете като кадри с маймуни или примати, въпреки че видеоклиповете нямат нищо общо с нито едно от двете.</p>
<p>Как се случва това? Пристрастието в изкуствения интелект обикновено е резултат от необективен набор за обучение. Както обяснява Джой Буоламвини в <a href="https://www.ted.com/talks/joy_buolamwini_how_i_m_fighting_bias_in_algorithms#t-257110" target="_blank">лекцията си в TED</a>, където обяснява как софтуерът за разпознаване на лица не е могъл да разпознае лицето ѝ. По същия начин набор от данни <a href="https://www.nytimes.com/2018/02/09/technology/facial-recognition-race-artificial-intelligence.html" target="_blank">където над 80 % от лицата са бели</a> може да има по-трудно разпознаване на хора с различен цвят на кожата и да прибегне до класифициране на тези лица като нещо близко до човека - като маймуни и други примати.</p>
<p>Подобен скандал на неотдавнашната грешка на Facebook беше наблюдаван през 2015 г., когато Google <a href="https://eu.usatoday.com/story/tech/2015/07/01/google-apologizes-after-photos-identify-black-people-as-gorillas/29567465/">погрешно идентифицира чернокожи хора като горили в Google Photos</a>. Вместо обаче да промени изкуствения интелект, <a href="https://www.wired.com/story/when-it-comes-to-gorillas-google-photos-remains-blind/" target="_blank">Google Photos се отърва от думи като "горила", "шимпанзе", "шимпанзе" и "маймуна". Въпреки че това означава, че хората вече няма да бъдат идентифицирани като маймуни в Google Photos, то предполага, че основните проблеми с изкуствения интелект все още не са отстранени.</a></p>
<h2>Расовото неправомерно поведение на Facebook</h2>
<p>Въпреки че грешката на изкуствения интелект на Facebook идва от основен проблем, който играе на обществено ниво, Facebook има опит със злоупотреби, често на расова основа.</p>
<ul>
<li>Потребителите на Инстаграм, чиято активност е подсказвала, че са чернокожи, са били с 50% по-застрашени от автоматично деактивиране на профилите им. <a href="https://www.nbcnews.com/tech/tech-news/facebook-management-ignored-internal-research-showing-racial-bias-current-former-n1234746" target="_blank">Изследователите са били помолени от своите началници да спрат изследването и да си мълчат за него</a>;</li>
<li>Марк Зукърбърг, главен изпълнителен директор на Facebook, <a href="https://gizmodo.com/mark-zuckerberg-asks-racist-facebook-employees-to-stop-1761272768" target="_blank">трябваше публично да помоли служителите си да спрат да зачеркват лозунгите Black Lives Matter</a>;</li>
<li>Фейсбук <a href="https://www.nytimes.com/2021/08/11/technology/facebook-soccer-racism.html" target="_blank">не успя да спре расистките обиди към английските футболисти</a>;</li>
<li>Няколко ужасяващи <a href="https://medium.com/@blindfb2020/facebook-empowers-racism-against-its-employees-of-color-fbbfaf55ab76" target="_blank">истории за расизъм и фанатизъм сред служителите на Facebook</a> бяха съобщени анонимно.</li>
</ul>
<p>Подобни примери оставят достатъчно място да се чудим колко искрено Facebook приема несъответствието с примата - и да се запитаме дали извинението не е просто PR акция за предотвратяване на по-нататъшни спорове.</p>
<h2>Какво можем да направим по въпроса?</h2>
<p>Разпознаването на образи е много полезен инструмент, който може да ни помогне да подобрим ежедневието си, но иновациите не трябва да са свързани с дискриминация или засилване на фанатизма. Класифицирането на човек като маймуна е такова унижение и не е нищо друго освен очевидно, че Facebook като компания трябва да носи отговорност.</p>
<p>Тривиално е, че един прост алгоритъм може да доведе до сериозни последици за голяма платформа, и следователно алгоритмите трябва да се третират по този начин. Алгоритъмът за разпознаване на лица не бива да бъде просто нещо, което може да се хвърли върху милиони хора, а категоризирането на хиляди хора в раздела за горили е също толкова неприемливо, колкото и публичното заклеймяване на тези хора като маймуни.</p>
<p>Не позволявайте извинението да е достатъчно, особено за компания с такова досие на расови злоупотреби.</p>
</article>
</section>
<footer>
<p>Този уебсайт е създаден от <a href="https://noordstar.me/">BramvdnHeuvel</a>.<br/> Можете да се свържете с тях на <a href="mailto:digital-rights@bram.blmgroep.nl">електронна поща</a> или на <a href="https://matrix.to/#/@bramvdnheuvel:nltrix.net">Матрикс</a>.</p>
<script src="https://digital-justice.com/js/expand-iframes.js"></script>
<script src="https://digital-justice.com/js/language-picker.js"></script>
</footer>
</body>
</html>
