<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<!-- Open Sans font -->
<link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:200,200i,300,300i,400,400i,600,600i,700,700i,900,900i|Open+Sans:300,300i,400,400i,600,600i,700,700i,800,800i" id="u-theme-google-font" rel="stylesheet"/>
<!-- Playfair Display font-->
<link href="https://fonts.googleapis.com/css?family=Playfair+Display:400,400i,500,500i,600,600i,700,700i,800,800i,900,900i" id="u-page-google-font" rel="stylesheet"/>
<!-- CSS -->
<link href="https://digital-justice.com/all.css" rel="stylesheet" type="text/css"/>
<link href="https://digital-justice.com/small.css" media="screen and (max-width: 48em)" rel="stylesheet"/>
<!-- Meta tags -->
<!-- Primary Meta Tags -->
<title>Facebook przeprasza za zaklasyfikowanie czarnych mężczyzn jako naczelnych.</title>
<meta content="Facebook przeprasza za zaklasyfikowanie czarnych mężczyzn jako naczelnych." name="title"/>
<meta content="Nigdy nie można zagwarantować, że AI nie dostarczy jakichś okropnych efektów ubocznych, ale historia nadużyć rasowych Facebooka sprawia, że ten błąd nie będzie ich ostatnim." name="description"/>
<!-- FontAwesome -->
<link href="https://digital-justice.com/icons/fontawesome-free-5.13.0-web/css/all.css" rel="stylesheet" type="text/css"/>
<!-- Open Graph / Facebook -->
<meta content="website" property="og:type"/>
<meta content="https://digital-justice.com/" property="og:url"/>
<meta content="Facebook przeprasza za zaklasyfikowanie czarnych mężczyzn jako naczelnych." property="og:title"/>
<meta content="Nigdy nie można zagwarantować, że AI nie dostarczy jakichś okropnych efektów ubocznych, ale historia nadużyć rasowych Facebooka sprawia, że ten błąd nie będzie ich ostatnim." property="og:description"/>
<meta content="https://digital-justice.com/images/data-sovereignty-nologo.png" property="og:image"/>
<!-- Twitter -->
<meta content="summary_large_image" property="twitter:card"/>
<meta content="https://digital-justice.com/" property="twitter:url"/>
<meta content="Facebook przeprasza za zaklasyfikowanie czarnych mężczyzn jako naczelnych." property="twitter:title"/>
<meta content="Nigdy nie można zagwarantować, że AI nie dostarczy jakichś okropnych efektów ubocznych, ale historia nadużyć rasowych Facebooka sprawia, że ten błąd nie będzie ich ostatnim." property="twitter:description"/>
<meta content="https://digital-justice.com/images/data-sovereignty-nologo.png" property="twitter:image"/>
<!-- Favicon -->
<!-- Favicon -->
<link href="https://digital-justice.com/icons/apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180"/>
<link href="https://digital-justice.com/icons/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png"/>
<link href="https://digital-justice.com/icons/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png"/>
<link href="https://digital-justice.com/icons/site.webmanifest" rel="manifest"/>
</head>
<body>
<header>
<nav>
<a href="../index.html"><i class="fas fa-home"></i> Strona główna</a>
<a href="../rights.html">Prawa cyfrowe</a>
<a href="index.html">Artykuły</a>
<div id="language-picker"></div>
<a class="hamburger" href="javascript:void(0)" onclick="openHamburger()">
<i class="fa fa-bars"></i>
</a>
</nav>
<script src="https://digital-justice.com/js/open-hamburger.js"></script>
</header>
<main>
<section>
<article>
<h1>Facebook przeprasza za zaklasyfikowanie czarnych mężczyzn jako naczelnych.</h1>
<p>To jest autentyczny artykuł napisany przez <a href="https://matrix.to/#/@bramvdnheuvel:nltrix.net">BramvdnHeuvel</a>.</p>
<p>Szacowany czas czytania: <img class="icon" src="https://digital-justice.com/images/clock.svg"/> 2 min.</p>
</article>
</section>
<section>
<article>
<div>
<h1>Facebook przeprasza za zaklasyfikowanie czarnych mężczyzn jako naczelnych.</h1>
<p>Sztuczne inteligencje mogą mieć uprzedzenia rasowe. <a href="https://www.nytimes.com/2019/12/19/technology/facial-recognition-bias.html" target="_blank">Twarze kaukaskie są od 10 do 100 razy bardziej prawdopodobne, że zostaną poprawnie rozpoznane i zidentyfikowane</a> niż twarze afroamerykańskie i azjatyckie, co już doprowadziło do <a href="https://www.nytimes.com/2020/06/24/technology/facial-recognition-arrest.html" target="_blank">nieprawdziwych oskarżeń, aresztowań, a nawet uwięzień</a>, ponieważ sztuczna inteligencja nie mogła poprawnie dopasować twarzy nie-białego.</p>
<p>Wielu studentów doświadczyło tego z pierwszej ręki podczas pandemii. Wiele firm takich jak <a href="https://proctorio.com/" target="_blank">Proctorio</a> odniosło duży sukces na uniwersytetach i w szkołach, gdzie nauczyciele instruują swoich studentów, aby zainstalowali oprogramowanie, które sprawdza, czy uczciwie wykonują test. Oprogramowanie było bardziej skłonne do obwiniania osób kolorowych o oszukiwanie, ponieważ <a href="https://micky.com.au/proctorio-test-software-fails-to-detect-people-of-color/" target="_blank">oprogramowanie nie mogło rozpoznać ich twarzy</a>. Takie oprogramowanie proctoring pokazał do <a href="https://www.technologyreview.com/2020/08/07/1006132/software-algorithms-proctoring-online-tests-ai-ethics/" target="_blank">reinforce białej supremacji, seksizm, ableism i transhobia</a>.</p>
<h2>Błąd Facebooka</h2>
<p>Przeprosiny Facebooka dotyczyły czarnych mężczyzn w starciach z białymi cywilami i policjantami, <a href="https://www.nytimes.com/2021/09/03/technology/facebook-ai-race-primates.html" target="_blank">według The New York Times</a>. Sztuczna inteligencja rozpoznająca obraz sklasyfikowała klipy jako nagrania małp lub naczelnych, mimo że filmy nie miały nic wspólnego z żadnym z nich.</p>
<p>Jak to się dzieje? Uprzedzenie w sztucznej inteligencji jest zazwyczaj wynikiem nieobiektywnego zestawu szkoleniowego. Jak wyjaśnia Joy Buolamwini w <a href="https://www.ted.com/talks/joy_buolamwini_how_i_m_fighting_bias_in_algorithms#t-257110" target="_blank">jej TED talk</a>, gdzie wyjaśnia, jak oprogramowanie do rozpoznawania twarzy nie mogło rozpoznać jej twarzy. W ten sam sposób, zbiór danych <a href="https://www.nytimes.com/2018/02/09/technology/facial-recognition-race-artificial-intelligence.html" target="_blank">gdzie ponad 80 procent twarzy jest białych</a> może mieć trudniejszy czas na rozpoznanie ludzi o różnych kolorach skóry i może uciekać się do klasyfikowania tych twarzy jako coś bliskiego człowiekowi - jak małpy i inne naczelne.</p>
<p>Podobny skandal do ostatniej pomyłki Facebooka był widoczny w 2015 roku, kiedy to Google <a href="https://eu.usatoday.com/story/tech/2015/07/01/google-apologizes-after-photos-identify-black-people-as-gorillas/29567465/">błędnie zidentyfikował czarnych ludzi jako goryle w Google Photos</a>. Jednak zamiast zmieniać sztuczną inteligencję, <a href="https://www.wired.com/story/when-it-comes-to-gorillas-google-photos-remains-blind/" target="_blank">Google Photos pozbyło się słów takich jak "goryl", "szympans", "szympans" i "małpa". Chociaż oznacza to, że ludzie nie będą już identyfikowani jako małpy na Zdjęciach Google, sugeruje to, że podstawowe problemy z SI nie zostały jeszcze naprawione.</a></p>
<h2>Rasistowskie wykroczenia Facebooka</h2>
<p>Chociaż błąd sztucznej inteligencji Facebooka wynika z problemu, który rozgrywa się na poziomie społecznym, Facebook ma na swoim koncie nadużycia, często na tle rasowym.</p>
<ul>
<li>Użytkownicy Instagrama, których aktywność sugerowała, że są czarni, mieli o 50% większe szanse na automatyczne wyłączenie konta. <a href="https://www.nbcnews.com/tech/tech-news/facebook-management-ignored-internal-research-showing-racial-bias-current-former-n1234746" target="_blank">Badacze zostali poinformowani przez swoich przełożonych, aby zaprzestali badań i milczeli na ten temat</a>;</li>
<li>Mark Zuckerberg, dyrektor generalny Facebooka, <a href="https://gizmodo.com/mark-zuckerberg-asks-racist-facebook-employees-to-stop-1761272768" target="_blank">musiał publicznie poprosić pracowników o zaprzestanie skreślania haseł Black Lives Matter</a>;</li>
<li>Facebook <a href="https://www.nytimes.com/2021/08/11/technology/facebook-soccer-racism.html" target="_blank">nie zdołał powstrzymać rasistowskich nadużyć wobec angielskich piłkarzy</a>;</li>
<li>Kilka przerażających <a href="https://medium.com/@blindfb2020/facebook-empowers-racism-against-its-employees-of-color-fbbfaf55ab76" target="_blank">historii rasizmu i bigoterii wśród pracowników Facebooka</a> zostało zgłoszonych anonimowo.</li>
</ul>
<p>Przykłady takie jak te pozostawiają wiele miejsca na zastanawianie się, jak szczerze Facebook traktuje niedopasowanie naczelnych -- i zastanawianie się, czy przeprosiny są po prostu działaniem PR, aby zapobiec dalszym kontrowersjom.</p>
<h2>Co możemy z tym zrobić?</h2>
<p>Rozpoznawanie obrazów jest bardzo użytecznym narzędziem, które może pomóc nam poprawić nasze codzienne życie, ale innowacyjność nie może iść w parze z dyskryminacją lub wzmacnianiem bigoterii. Zaklasyfikowanie człowieka jako małpy jest tak haniebne i nie jest niczym innym, jak tylko oczywistym, że Facebook jako firma powinien być pociągnięty do odpowiedzialności.</p>
<p>To banalne, że prosty algorytm może prowadzić do poważnych konsekwencji na dużej platformie, i dlatego algorytmy powinny być traktowane w ten sposób. Algorytm rozpoznawania twarzy nie powinien być po prostu czymś, co można po prostu wrzucić na miliony ludzi, a zakwalifikowanie tysięcy ludzi do sekcji goryli jest tak samo nieakceptowalne jak publiczne potępianie tych ludzi jako małpy.</p>
<p>Nie pozwól, by przeprosiny wystarczyły, zwłaszcza w przypadku firmy z taką historią nadużyć na tle rasowym.</p>
</div>
</article>
</section>
</main>
<footer>
<p>Ta strona została założona przez <a href="https://noordstar.me/">BramvdnHeuvel</a>. Tutaj jest <a href="https://github.com/BramvdnHeuvel/Digital-Justice">kod źródłowy.</a><br/> Możesz skontaktować się z nimi przez <a href="mailto:hello@digital-justice.com">email</a> lub na <a href="https://matrix.to/#/#digital-justice:noordstar.me">Matrix</a>.</p>
<script src="https://digital-justice.com/js/expand-iframes.js"></script>
<script src="https://digital-justice.com/js/language-picker.js"></script>
</footer>
</body>
</html>
