<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<!-- Open Sans font -->
<link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:200,200i,300,300i,400,400i,600,600i,700,700i,900,900i|Open+Sans:300,300i,400,400i,600,600i,700,700i,800,800i" id="u-theme-google-font" rel="stylesheet"/>
<!-- Playfair Display font-->
<link href="https://fonts.googleapis.com/css?family=Playfair+Display:400,400i,500,500i,600,600i,700,700i,800,800i,900,900i" id="u-page-google-font" rel="stylesheet"/>
<!-- CSS -->
<link href="https://digital-justice.com/all.css" rel="stylesheet" type="text/css"/>
<link href="https://digital-justice.com/small.css" media="screen and (max-width: 48em)" rel="stylesheet"/>
<!-- Meta tags -->
<!-- Primary Meta Tags -->
<title>Facebook s'excuse d'avoir classé les hommes noirs comme des primates</title>
<meta content="Facebook s'excuse d'avoir classé les hommes noirs comme des primates" name="title"/>
<meta content="On ne peut jamais garantir qu'une IA n'aura pas d'effets secondaires terribles, mais les antécédents de Facebook en matière d'abus raciaux laissent penser que cette erreur ne sera pas la dernière." name="description"/>
<!-- FontAwesome -->
<link href="https://digital-justice.com/icons/fontawesome-free-5.13.0-web/css/all.css" rel="stylesheet" type="text/css"/>
<!-- Open Graph / Facebook -->
<meta content="website" property="og:type"/>
<meta content="https://digital-justice.com/" property="og:url"/>
<meta content="Facebook s'excuse d'avoir classé les hommes noirs comme des primates" property="og:title"/>
<meta content="On ne peut jamais garantir qu'une IA n'aura pas d'effets secondaires terribles, mais les antécédents de Facebook en matière d'abus raciaux laissent penser que cette erreur ne sera pas la dernière." property="og:description"/>
<meta content="https://digital-justice.com/images/data-sovereignty-nologo.png" property="og:image"/>
<!-- Twitter -->
<meta content="summary_large_image" property="twitter:card"/>
<meta content="https://digital-justice.com/" property="twitter:url"/>
<meta content="Facebook s'excuse d'avoir classé les hommes noirs comme des primates" property="twitter:title"/>
<meta content="On ne peut jamais garantir qu'une IA n'aura pas d'effets secondaires terribles, mais les antécédents de Facebook en matière d'abus raciaux laissent penser que cette erreur ne sera pas la dernière." property="twitter:description"/>
<meta content="https://digital-justice.com/images/data-sovereignty-nologo.png" property="twitter:image"/>
<!-- Favicon -->
<!-- Favicon -->
<link href="https://digital-justice.com/icons/apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180"/>
<link href="https://digital-justice.com/icons/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png"/>
<link href="https://digital-justice.com/icons/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png"/>
<link href="https://digital-justice.com/icons/site.webmanifest" rel="manifest"/>
</head>
<body>
<header>
<nav>
<a href="../index.html"><i class="fas fa-home"></i> Accueil</a>
<a href="../rights.html">Droits numériques</a>
<a href="index.html">Articles</a>
<div id="language-picker"></div>
<a class="hamburger" href="javascript:void(0)" onclick="openHamburger()">
<i class="fa fa-bars"></i>
</a>
</nav>
<script src="https://digital-justice.com/js/open-hamburger.js"></script>
</header>
<main>
<section>
<article>
<h1>Facebook s'excuse d'avoir classé les hommes noirs comme des primates</h1>
<p>Ceci est un article authentique écrit par <a href="https://matrix.to/#/@bramvdnheuvel:nltrix.net">BramvdnHeuvel</a>.</p>
<p>Temps de lecture estimé : <img class="icon" src="https://digital-justice.com/images/clock.svg"/> 2 minutes.</p>
</article>
</section>
<section>
<article>
<div>
<h1>Facebook s'excuse d'avoir classé les hommes noirs comme des primates</h1>
<p>Les intelligences artificielles peuvent avoir des préjugés raciaux. <a href="https://www.nytimes.com/2019/12/19/technology/facial-recognition-bias.html" target="_blank">Les visages caucasiens ont 10 à 100 fois plus de chances d'être correctement reconnus et identifiés</a> que les visages afro-américains et asiatiques, ce qui a déjà conduit à <a href="https://www.nytimes.com/2020/06/24/technology/facial-recognition-arrest.html" target="_blank">des accusations injustifiées, des arrestations et même des emprisonnements</a> parce qu'une intelligence artificielle ne pouvait pas faire correspondre correctement un visage non blanc.</p>
<p>De nombreux étudiants en ont fait l'expérience directe pendant la pandémie. Des entreprises comme <a href="https://proctorio.com/" target="_blank">Proctorio</a> ont connu un grand succès dans les universités et les écoles, où les enseignants demandent à leurs élèves d'installer un logiciel qui vérifie s'ils font un test honnêtement. Le logiciel était plus susceptible d'accuser les personnes de couleur de tricher parce que <a href="https://micky.com.au/proctorio-test-software-fails-to-detect-people-of-color/" target="_blank">le logiciel ne pouvait pas reconnaître leur visage</a>. De tels logiciels de surveillance ont montré qu'ils <a href="https://www.technologyreview.com/2020/08/07/1006132/software-algorithms-proctoring-online-tests-ai-ethics/" target="_blank">renforcent la suprématie blanche, le sexisme, le capacitisme et la transphobie</a>.</p>
<h2>L'erreur de Facebook</h2>
<p>Les excuses de Facebook concernaient des hommes noirs dans des altercations avec des civils et des policiers blancs, <a href="https://www.nytimes.com/2021/09/03/technology/facebook-ai-race-primates.html" target="_blank">selon le New York Times</a>. Une IA de reconnaissance d'image a classé les clips comme des séquences de singes ou de primates, même si les vidéos n'avaient rien à voir avec l'un ou l'autre.</p>
<p>Comment cela se produit-il ? Un biais dans une intelligence artificielle est généralement le résultat d'un ensemble de formation biaisé. Comme l'explique Joy Buolamwini dans <a href="https://www.ted.com/talks/joy_buolamwini_how_i_m_fighting_bias_in_algorithms#t-257110" target="_blank">son TED talk</a>, où elle explique comment un logiciel de reconnaissance faciale ne pouvait pas reconnaître son visage. De la même manière, un ensemble de données <a href="https://www.nytimes.com/2018/02/09/technology/facial-recognition-race-artificial-intelligence.html" target="_blank">où plus de 80 % des visages est blanc</a> peut avoir plus de mal à reconnaître les personnes de différentes couleurs de peau et peut se résoudre à classer ce visage comme quelque chose de proche de l'humain - comme les singes et autres primates.</p>
<p>Un scandale similaire à la récente erreur de Facebook a été observé en 2015, lorsque Google <a href="https://eu.usatoday.com/story/tech/2015/07/01/google-apologizes-after-photos-identify-black-people-as-gorillas/29567465/">a identifié par erreur des personnes noires comme des gorilles dans Google Photos</a>. Cependant, au lieu de modifier l'intelligence artificielle, <a href="https://www.wired.com/story/when-it-comes-to-gorillas-google-photos-remains-blind/" target="_blank">Google Photos s'est débarrassé de mots comme "gorille", "chimpanzé", "chimpanzé" et "singe". Bien que cela signifie que les humains ne seront plus identifiés comme des singes sur Google Photos, cela suggère que les problèmes sous-jacents de l'IA n'ont pas encore été réglés.</a></p>
<h2>L'inconduite raciale de Facebook</h2>
<p>Bien que l'erreur de l'IA de Facebook provienne d'un problème sous-jacent qui joue à un niveau sociétal, Facebook a un historique d'abus, souvent raciaux.</p>
<ul>
<li>Les utilisateurs d'Instagram dont l'activité suggérait qu'ils étaient noirs étaient 50% plus susceptibles de voir leur compte automatiquement désactivé. <a href="https://www.nbcnews.com/tech/tech-news/facebook-management-ignored-internal-research-showing-racial-bias-current-former-n1234746" target="_blank">Les chercheurs ont été priés par leurs supérieurs d'arrêter les recherches et de garder le silence à ce sujet</a> ;</li>
<li>Mark Zuckerberg, le PDG de Facebook, <a href="https://gizmodo.com/mark-zuckerberg-asks-racist-facebook-employees-to-stop-1761272768" target="_blank">a dû demander publiquement aux employés d'arrêter de rayer les slogans de Black Lives Matter</a> ;</li>
<li>Facebook <a href="https://www.nytimes.com/2021/08/11/technology/facebook-soccer-racism.html" target="_blank">n'a pas réussi à endiguer les insultes racistes envers les joueurs de football anglais</a> ;</li>
<li>Plusieurs <a href="https://medium.com/@blindfb2020/facebook-empowers-racism-against-its-employees-of-color-fbbfaf55ab76" target="_blank">histoires horribles de racisme et de bigoterie parmi les employés de Facebook</a> ont été rapportées anonymement.</li>
</ul>
<p>Des exemples comme ceux-ci laissent beaucoup de place pour se demander à quel point Facebook prend sincèrement l'erreur d'appariement des primates - et pour se demander si les excuses sont simplement une action de relations publiques pour éviter toute autre controverse.</p>
<h2>Que pouvons-nous faire à ce sujet ?</h2>
<p>La reconnaissance d'images est un outil très utile qui peut nous aider à améliorer notre vie quotidienne, mais l'innovation ne doit pas s'accompagner de discrimination ou du renforcement du sectarisme. La classification d'un humain en tant que singe est d'une telle indignité, et il est tout à fait évident que Facebook, en tant qu'entreprise, doit être tenu pour responsable.</p>
<p>Il n'est que trivial qu'un simple algorithme puisse entraîner des conséquences majeures sur une grande plate-forme, et les algorithmes devraient donc être traités de cette manière. Un algorithme de reconnaissance faciale ne devrait pas être quelque chose que l'on peut simplement jeter sur des millions de personnes, et classer des milliers de personnes dans la section des gorilles est aussi inacceptable que de dénoncer publiquement ces personnes comme des singes.</p>
<p>Les excuses ne sont pas suffisantes, surtout pour une entreprise ayant un tel historique d'abus raciaux.</p>
</div>
</article>
</section>
</main>
<footer>
<p>Ce site web a été fondé par <a href="https://noordstar.me/">BramvdnHeuvel</a>. Voici <a href="https://github.com/BramvdnHeuvel/Digital-Justice">le code source.</a><br/>Vous pouvez les contacter par <a href="mailto:hello@digital-justice.com">email</a> ou sur <a href="https://matrix.to/#/#digital-justice:noordstar.me">Matrix</a>.</p>
<script src="https://digital-justice.com/js/expand-iframes.js"></script>
<script src="https://digital-justice.com/js/language-picker.js"></script>
</footer>
</body>
</html>
