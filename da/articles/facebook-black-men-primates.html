<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<!-- Open Sans font -->
<link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:200,200i,300,300i,400,400i,600,600i,700,700i,900,900i|Open+Sans:300,300i,400,400i,600,600i,700,700i,800,800i" id="u-theme-google-font" rel="stylesheet"/>
<!-- Playfair Display font-->
<link href="https://fonts.googleapis.com/css?family=Playfair+Display:400,400i,500,500i,600,600i,700,700i,800,800i,900,900i" id="u-page-google-font" rel="stylesheet"/>
<!-- CSS -->
<link href="https://digital-justice.com/all.css" rel="stylesheet" type="text/css"/>
<link href="https://digital-justice.com/small.css" media="screen and (max-width: 48em)" rel="stylesheet"/>
<!-- Meta tags -->
<!-- Primary Meta Tags -->
<title>Facebook undskylder for at klassificere sorte mænd som primater</title>
<meta content="Facebook undskylder for at klassificere sorte mænd som primater" name="title"/>
<meta content="Man kan aldrig garantere, at en AI leverer nogle forfærdelige bivirkninger, men Facebooks historie med racistiske overgreb gør det sandsynligt, at denne fejl ikke bliver deres sidste." name="description"/>
<!-- FontAwesome -->
<link href="https://digital-justice.com/icons/fontawesome-free-5.13.0-web/css/all.css" rel="stylesheet" type="text/css"/>
<!-- Open Graph / Facebook -->
<meta content="website" property="og:type"/>
<meta content="https://digital-justice.com/" property="og:url"/>
<meta content="Facebook undskylder for at klassificere sorte mænd som primater" property="og:title"/>
<meta content="Man kan aldrig garantere, at en AI leverer nogle forfærdelige bivirkninger, men Facebooks historie med racistiske overgreb gør det sandsynligt, at denne fejl ikke bliver deres sidste." property="og:description"/>
<meta content="https://digital-justice.com/images/data-sovereignty-nologo.png" property="og:image"/>
<!-- Twitter -->
<meta content="summary_large_image" property="twitter:card"/>
<meta content="https://digital-justice.com/" property="twitter:url"/>
<meta content="Facebook undskylder for at klassificere sorte mænd som primater" property="twitter:title"/>
<meta content="Man kan aldrig garantere, at en AI leverer nogle forfærdelige bivirkninger, men Facebooks historie med racistiske overgreb gør det sandsynligt, at denne fejl ikke bliver deres sidste." property="twitter:description"/>
<meta content="https://digital-justice.com/images/data-sovereignty-nologo.png" property="twitter:image"/>
<!-- Favicon -->
<!-- Favicon -->
<link href="https://digital-justice.com/icons/apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180"/>
<link href="https://digital-justice.com/icons/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png"/>
<link href="https://digital-justice.com/icons/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png"/>
<link href="https://digital-justice.com/icons/site.webmanifest" rel="manifest"/>
</head>
<body>
<header>
<nav>
<a href="../index.html"><i class="fas fa-home"></i> Forside</a>
<a href="../rights.html">Digitale rettigheder</a>
<a href="index.html">Artikler</a>
<div id="language-picker"></div>
<a class="hamburger" href="javascript:void(0)" onclick="openHamburger()">
<i class="fa fa-bars"></i>
</a>
</nav>
<script src="https://digital-justice.com/js/open-hamburger.js"></script>
</header>
<main>
<section>
<article>
<h1>Facebook undskylder for at klassificere sorte mænd som primater</h1>
<p>Dette er en autentisk artikel skrevet af <a href="https://matrix.to/#/@bramvdnheuvel:nltrix.net">BramvdnHeuvel</a>.</p>
<p>Anslået læsetid: <img class="icon" src="https://digital-justice.com/images/clock.svg"/> 2 minutter.</p>
</article>
</section>
<section>
<article>
<div>
<h1>Facebook undskylder for at klassificere sorte mænd som primater</h1>
<p>Kunstige intelligenser kan have racemæssige fordomme. <a href="https://www.nytimes.com/2019/12/19/technology/facial-recognition-bias.html" target="_blank">Caukasiske ansigter er 10 til 100 gange mere tilbøjelige til at blive genkendt og identificeret korrekt</a> end afroamerikanske ansigter og asiatiske ansigter, hvilket allerede har ført til <a href="https://www.nytimes.com/2020/06/24/technology/facial-recognition-arrest.html" target="_blank">forkerte anklager, arrestationer og endda fængslinger</a>, fordi en kunstig intelligens ikke kunne matche et ikke-hvidt ansigt korrekt.</p>
<p>Mange studerende har oplevet dette på egen krop under pandemien. Mange virksomheder som <a href="https://proctorio.com/" target="_blank">Proctorio</a> har gjort store succeser på universiteter og skoler, hvor lærere instruerer deres elever i at installere software, der kontrollerer, om de laver en prøve ærligt. Softwaren var mere tilbøjelig til at beskylde farvede personer for at snyde, fordi <a href="https://micky.com.au/proctorio-test-software-fails-to-detect-people-of-color/" target="_blank">softwaren ikke kunne genkende deres ansigter</a>. Det har vist sig, at en sådan software til kontrol af prøverne <a href="https://www.technologyreview.com/2020/08/07/1006132/software-algorithms-proctoring-online-tests-ai-ethics/" target="_blank">forstærker hvidt overherredømme, sexisme, handicap og transfobi</a>.</p>
<h2>Facebooks fejltagelse</h2>
<p>Facebooks undskyldning vedrørte sorte mænd i sammenstød med hvide civile og politibetjente, <a href="https://www.nytimes.com/2021/09/03/technology/facebook-ai-race-primates.html" target="_blank"> ifølge New York Times</a>. En AI til billedgenkendelse klassificerede klippene som optagelser af aber eller primater, selv om videoerne ikke havde noget med nogen af delene at gøre.</p>
<p>Hvordan kan dette ske? En skævhed i en kunstig intelligens er normalt et resultat af et skævt træningssæt. Som Joy Buolamwini forklarer i <a href="https://www.ted.com/talks/joy_buolamwini_how_i_m_fighting_bias_in_algorithms#t-257110" target="_blank">hendes TED-talk</a>, hvor hun forklarer, hvordan ansigtsgenkendelsessoftware ikke kunne genkende hendes ansigt. På samme måde kan et datasæt <a href="https://www.nytimes.com/2018/02/09/technology/facial-recognition-race-artificial-intelligence.html" target="_blank">hvor over 80 procent af ansigterne er hvide</a> have sværere ved at genkende mennesker med forskellige hudfarver og kan ty til at klassificere disse ansigter som noget, der ligger tæt på et menneske - som aber og andre primater.</p>
<p>En lignende skandale som Facebooks nylige fejltagelse blev set i 2015, da Google <a href="https://eu.usatoday.com/story/tech/2015/07/01/google-apologizes-after-photos-identify-black-people-as-gorillas/29567465/">fejlvis identificerede sorte mennesker som gorillaer i Google Photos</a>. I stedet for at ændre den kunstige intelligens fik <a href="https://www.wired.com/story/when-it-comes-to-gorillas-google-photos-remains-blind/" target="_blank">Google Photos dog fjernet ord som "gorilla", "chimpanse", "chimpanse" og "abe". Selv om det betyder, at mennesker ikke længere vil blive identificeret som aber på Google Photos, tyder det på, at de underliggende problemer med den kunstige intelligens endnu ikke er blevet løst.</a></p>
<h2>Facebooks racistiske forseelser</h2>
<p>Selv om Facebook AI's fejl kommer fra et underliggende problem, der spiller på et samfundsmæssigt niveau, har Facebook en rekord af misbrug, ofte racistisk.</p>
<ul>
<li>Instagram-brugere, hvis aktivitet tydede på, at de var sorte, havde 50 % større sandsynlighed for at få deres konti automatisk deaktiveret. <a href="https://www.nbcnews.com/tech/tech-news/facebook-management-ignored-internal-research-showing-racial-bias-current-former-n1234746" target="_blank">Forskerne fik besked fra deres overordnede om at stoppe forskningen og tie stille om den</a>;</li>
<li>Mark Zuckerberg, Facebooks administrerende direktør, <a href="https://gizmodo.com/mark-zuckerberg-asks-racist-facebook-employees-to-stop-1761272768" target="_blank">blev nødt til offentligt at bede medarbejderne om at holde op med at strege Black Lives Matter-slogans ud</a>;</li>
<li>Facebook <a href="https://www.nytimes.com/2021/08/11/technology/facebook-soccer-racism.html" target="_blank">forsømte at dæmme op for racistiske krænkelser af Englands fodboldspillere</a>;</li>
<li>Flere forfærdelige <a href="https://medium.com/@blindfb202020/facebook-empowers-racism-against-its-employees-of-color-fbbfaf55ab76" target="_blank">historier om racisme og bigotteri blandt Facebooks medarbejdere</a> er blevet rapporteret anonymt.</li>
</ul>
<p>Eksempler som disse giver masser af plads til at spekulere på, hvor oprigtigt Facebook tager primatfejlmatchen oprigtigt - og til at spekulere på, om undskyldningen blot er en PR-aktion for at forhindre yderligere kontroverser.</p>
<h2>Hvad kan vi gøre ved det?</h2>
<p>Billedgenkendelse er et meget nyttigt værktøj, der kan hjælpe os med at forbedre vores dagligdag, men innovation må ikke ledsages af diskrimination eller forstærkning af fanatisme. Klassificeringen af et menneske som en abe er af en sådan uværdighed, og det er intet andet end indlysende, at Facebook som virksomhed bør holdes ansvarlig.</p>
<p>Det er kun trivielt, at en simpel algoritme kan føre til store konsekvenser på en stor platform, og derfor bør algoritmer behandles på denne måde. En algoritme til ansigtsgenkendelse bør ikke bare være noget man bare kan smide på millioner af mennesker, og at kategorisere tusindvis af mennesker i gorillaafdelingen er lige så uacceptabelt som offentligt at fordømme disse mennesker som aber.</p>
<p>Lad ikke en undskyldning være nok, især ikke for en virksomhed med en sådan track record for racistisk misbrug.</p>
</div>
</article>
</section>
</main>
<footer>
<p>Dette websted blev grundlagt af <a href="https://noordstar.me/">BramvdnHeuvel</a>. Her er <a href="https://github.com/BramvdnHeuvel/Digital-Justice">kildekoden.</a><br/> Du kan kontakte dem pr <a href="mailto:hello@digital-justice.com">email</a> eller på <a href="https://matrix.to/#/#digital-justice:noordstar.me">Matrix</a>.</p>
<script src="https://digital-justice.com/js/expand-iframes.js"></script>
<script src="https://digital-justice.com/js/language-picker.js"></script>
</footer>
</body>
</html>
