<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<!-- Open Sans font -->
<link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:200,200i,300,300i,400,400i,600,600i,700,700i,900,900i|Open+Sans:300,300i,400,400i,600,600i,700,700i,800,800i" id="u-theme-google-font" rel="stylesheet"/>
<!-- Playfair Display font-->
<link href="https://fonts.googleapis.com/css?family=Playfair+Display:400,400i,500,500i,600,600i,700,700i,800,800i,900,900i" id="u-page-google-font" rel="stylesheet"/>
<!-- CSS -->
<link href="https://digital-justice.com/all.css" rel="stylesheet" type="text/css"/>
<!-- Meta tags -->
<!-- Primary Meta Tags -->
<title>Facebook si scusa per aver classificato gli uomini neri come primati</title>
<meta content="Facebook si scusa per aver classificato gli uomini neri come primati" name="title"/>
<meta content="Non si può mai garantire che un'IA produca dei terribili effetti collaterali, ma la storia di abusi razziali di Facebook rende probabile che questo errore non sarà l'ultimo." name="description"/>
<!-- FontAwesome -->
<link href="https://digital-justice.com/icons/fontawesome-free-5.13.0-web/css/all.css" rel="stylesheet" type="text/css"/>
<!-- Open Graph / Facebook -->
<meta content="website" property="og:type"/>
<meta content="https://digital-justice.com/" property="og:url"/>
<meta content="Facebook si scusa per aver classificato gli uomini neri come primati" property="og:title"/>
<meta content="Non si può mai garantire che un'IA produca dei terribili effetti collaterali, ma la storia di abusi razziali di Facebook rende probabile che questo errore non sarà l'ultimo." property="og:description"/>
<meta content="https://digital-justice.com/images/data-sovereignty-nologo.png" property="og:image"/>
<!-- Twitter -->
<meta content="summary_large_image" property="twitter:card"/>
<meta content="https://digital-justice.com/" property="twitter:url"/>
<meta content="Facebook si scusa per aver classificato gli uomini neri come primati" property="twitter:title"/>
<meta content="Non si può mai garantire che un'IA produca dei terribili effetti collaterali, ma la storia di abusi razziali di Facebook rende probabile che questo errore non sarà l'ultimo." property="twitter:description"/>
<meta content="https://digital-justice.com/images/data-sovereignty-nologo.png" property="twitter:image"/>
<!-- Favicon -->
<!-- Favicon -->
<link href="https://digital-justice.com/icons/apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180"/>
<link href="https://digital-justice.com/icons/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png"/>
<link href="https://digital-justice.com/icons/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png"/>
<link href="https://digital-justice.com/icons/site.webmanifest" rel="manifest"/>
</head>
<body>
<header>
<nav>
<a href="../index.html"><i class="fas fa-home"></i> Home</a>
<a href="../rights.html">Diritti digitali</a>
<a href="index.html">Articoli</a>
<div id="language-picker"></div>
<a class="hamburger" href="javascript:void(0)" onclick="openHamburger()">
<i class="fa fa-bars"></i>
</a>
</nav>
<script src="https://digital-justice.com/js/open-hamburger.js"></script>
</header>
<section class="simple-orange">
<article class="essay-title">
<h1>Facebook si scusa per aver classificato gli uomini neri come primati</h1>
<p>Questo è un articolo autentico scritto da <a href="https://matrix.to/#/@bramvdnheuvel:nltrix.net">BramvdnHeuvel</a>.</p>
<p>Tempo di lettura stimato: <img class="icon" src="https://digital-justice.com/images/clock.svg"/> 2 minuti.</p>
</article>
</section>
<section class="simple-gray flex-container">
<article class="blog">
<h1>Facebook si scusa per aver classificato gli uomini neri come primati</h1>
<p>Le intelligenze artificiali possono avere pregiudizi razziali. <a href="https://www.nytimes.com/2019/12/19/technology/facial-recognition-bias.html" target="_blank">I volti caucasici hanno da 10 a 100 volte più probabilità di essere correttamente riconosciuti e identificati</a> rispetto ai volti afro-americani e asiatici, il che ha già portato a <a href="https://www.nytimes.com/2020/06/24/technology/facial-recognition-arrest.html" target="_blank">accuse ingiuste, arresti e persino imprigionamenti</a> perché un'intelligenza artificiale non poteva abbinare correttamente un volto non bianco.</p>
<p>Molti studenti l'hanno sperimentato in prima persona durante la pandemia. Molte aziende come <a href="https://proctorio.com/" target="_blank">Proctorio</a> hanno fatto grandi successi nelle università e nelle scuole, dove gli insegnanti istruiscono i loro studenti ad installare un software che controlla se stanno facendo un test onestamente. Il software aveva maggiori probabilità di accusare le persone di colore di imbrogliare perché <a href="https://micky.com.au/proctorio-test-software-fails-to-detect-people-of-color/" target="_blank">il software non poteva riconoscere i loro volti</a>. Tale software di supervisione ha dimostrato di <a href="https://www.technologyreview.com/2020/08/07/1006132/software-algorithms-proctoring-online-tests-ai-ethics/" target="_blank">rafforzare la supremazia bianca, il sessismo, l'ableismo e la transfobia</a>.</p>
<h2>L'errore di Facebook</h2>
<p>Le scuse di Facebook riguardavano uomini neri in alterchi con civili bianchi e agenti di polizia, <a href="https://www.nytimes.com/2021/09/03/technology/facebook-ai-race-primates.html" target="_blank">secondo il New York Times</a>. Un'IA di riconoscimento delle immagini ha classificato le clip come filmati di scimmie o primati, anche se i video non avevano nulla a che fare con entrambi.</p>
<p>Come avviene questo? Una distorsione in un'intelligenza artificiale è di solito il risultato di un set di allenamento distorto. Come spiega Joy Buolamwini in <a href="https://www.ted.com/talks/joy_buolamwini_how_i_m_fighting_bias_in_algorithms#t-257110" target="_blank">il suo TED talk</a>, dove spiega come il software di riconoscimento facciale non ha potuto riconoscere il suo volto. Allo stesso modo, un set di dati <a href="https://www.nytimes.com/2018/02/09/technology/facial-recognition-race-artificial-intelligence.html" target="_blank">dove oltre l'80 per cento dei volti è bianco</a> può avere più difficoltà a riconoscere persone di diverso colore della pelle e può ricorrere a classificare quei volti come qualcosa di vicino a un umano - come scimmie e altri primati.</p>
<p>Uno scandalo simile al recente errore di Facebook è stato visto nel 2015, quando Google <a href="https://eu.usatoday.com/story/tech/2015/07/01/google-apologizes-after-photos-identify-black-people-as-gorillas/29567465/">ha erroneamente identificato persone di colore come gorilla in Google Photos</a>. Tuttavia, invece di cambiare l'intelligenza artificiale, <a href="https://www.wired.com/story/when-it-comes-to-gorillas-google-photos-remains-blind/" target="_blank">Google Photos si è sbarazzato di parole come "gorilla", "scimpanzé", "scimpanzé" e "scimmia". Anche se questo significa che gli umani non saranno più identificati come scimmie su Google Photos, suggerisce che i problemi di fondo con l'IA non sono stati ancora risolti.</a></p>
<h2>La cattiva condotta razziale di Facebook</h2>
<p>Anche se l'errore dell'IA di Facebook deriva da un problema di fondo che gioca a livello sociale, Facebook ha un record di abusi, spesso razziali.</p>
<ul>
<li>Gli utenti di Instagram la cui attività suggeriva che erano neri avevano il 50% di probabilità in più di avere i loro account automaticamente disabilitati. <a href="https://www.nbcnews.com/tech/tech-news/facebook-management-ignored-internal-research-showing-racial-bias-current-former-n1234746" target="_blank">I ricercatori hanno ricevuto dai loro superiori l'ordine di interrompere la ricerca e di non parlarne</a>;</li>
<li>Mark Zuckerberg, CEO di Facebook, <a href="https://gizmodo.com/mark-zuckerberg-asks-racist-facebook-employees-to-stop-1761272768" target="_blank">ha dovuto chiedere pubblicamente ai dipendenti di smettere di cancellare gli slogan di Black Lives Matter</a>;</li>
<li>Facebook <a href="https://www.nytimes.com/2021/08/11/technology/facebook-soccer-racism.html" target="_blank">non è riuscito ad arginare l'abuso razzista dei calciatori inglesi</a>;</li>
<li>Diverse orribili <a href="https://medium.com/@blindfb2020/facebook-empowers-racism-against-its-employees-of-color-fbbfaf55ab76" target="_blank">storie di razzismo e bigottismo tra i dipendenti di Facebook</a> sono state riportate in forma anonima.</li>
</ul>
<p>Esempi come questi lasciano molto spazio per chiedersi quanto sinceramente Facebook prenda il mismatch del primate -- e per chiedersi se le scuse siano semplicemente un'azione di PR per prevenire ulteriori controversie.</p>
<h2>Cosa possiamo fare al riguardo?</h2>
<p>Il riconoscimento delle immagini è uno strumento molto utile che può aiutarci a migliorare la nostra vita quotidiana, ma l'innovazione non deve venire con la discriminazione o il rafforzamento del bigottismo. La classificazione di un umano come scimmia è di una tale indegnità, e non è altro che ovvio che Facebook come azienda dovrebbe essere ritenuta responsabile.</p>
<p>È banale che un semplice algoritmo possa portare a conseguenze importanti su una grande piattaforma, e quindi gli algoritmi dovrebbero essere trattati in questo modo. Un algoritmo di riconoscimento facciale non dovrebbe essere semplicemente qualcosa che si può semplicemente gettare su milioni di persone, e categorizzare migliaia di persone nella sezione gorilla è inaccettabile quanto denunciare pubblicamente quelle persone come scimmie.</p>
<p>Non lasciare che le scuse siano sufficienti, specialmente per un'azienda con un tale record di abusi razziali.</p>
</article>
</section>
<footer>
<p>Questo sito è stato costruito da <a href="https://noordstar.me/">BramvdnHeuvel</a>.<br/> Puoi contattarli per <a href="mailto:digital-rights@bram.blmgroep.nl">email</a> o su <a href="https://matrix.to/#/@bramvdnheuvel:nltrix.net">Matrix</a>.</p>
<script src="https://digital-justice.com/js/expand-iframes.js"></script>
<script src="https://digital-justice.com/js/language-picker.js"></script>
</footer>
</body>
</html>
