<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<!-- Open Sans font -->
<link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:200,200i,300,300i,400,400i,600,600i,700,700i,900,900i|Open+Sans:300,300i,400,400i,600,600i,700,700i,800,800i" id="u-theme-google-font" rel="stylesheet"/>
<!-- Playfair Display font-->
<link href="https://fonts.googleapis.com/css?family=Playfair+Display:400,400i,500,500i,600,600i,700,700i,800,800i,900,900i" id="u-page-google-font" rel="stylesheet"/>
<!-- CSS -->
<link href="https://digital-justice.com/all.css" rel="stylesheet" type="text/css"/>
<link href="https://digital-justice.com/small.css" media="screen and (max-width: 48em)" rel="stylesheet"/>
<!-- Meta tags -->
<!-- Primary Meta Tags -->
<title>Facebook se disculpa por clasificar a los negros como primates</title>
<meta content="Facebook se disculpa por clasificar a los negros como primates" name="title"/>
<meta content="Nunca se puede garantizar que una IA produzca efectos secundarios horribles, pero el historial de abusos raciales de Facebook hace pensar que este error no será el último." name="description"/>
<!-- FontAwesome -->
<link href="https://digital-justice.com/icons/fontawesome-free-5.13.0-web/css/all.css" rel="stylesheet" type="text/css"/>
<!-- Open Graph / Facebook -->
<meta content="website" property="og:type"/>
<meta content="https://digital-justice.com/" property="og:url"/>
<meta content="Facebook se disculpa por clasificar a los negros como primates" property="og:title"/>
<meta content="Nunca se puede garantizar que una IA produzca efectos secundarios horribles, pero el historial de abusos raciales de Facebook hace pensar que este error no será el último." property="og:description"/>
<meta content="https://digital-justice.com/images/data-sovereignty-nologo.png" property="og:image"/>
<!-- Twitter -->
<meta content="summary_large_image" property="twitter:card"/>
<meta content="https://digital-justice.com/" property="twitter:url"/>
<meta content="Facebook se disculpa por clasificar a los negros como primates" property="twitter:title"/>
<meta content="Nunca se puede garantizar que una IA produzca efectos secundarios horribles, pero el historial de abusos raciales de Facebook hace pensar que este error no será el último." property="twitter:description"/>
<meta content="https://digital-justice.com/images/data-sovereignty-nologo.png" property="twitter:image"/>
<!-- Favicon -->
<!-- Favicon -->
<link href="https://digital-justice.com/icons/apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180"/>
<link href="https://digital-justice.com/icons/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png"/>
<link href="https://digital-justice.com/icons/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png"/>
<link href="https://digital-justice.com/icons/site.webmanifest" rel="manifest"/>
</head>
<body>
<header>
<nav>
<a href="../index.html"><i class="fas fa-home"></i> Inicio</a>
<a href="../rights.html">Derechos digitales</a>
<a href="index.html">Artículos</a>
<div id="language-picker"></div>
<a class="hamburger" href="javascript:void(0)" onclick="openHamburger()">
<i class="fa fa-bars"></i>
</a>
</nav>
<script src="https://digital-justice.com/js/open-hamburger.js"></script>
</header>
<main>
<section>
<article>
<h1>Facebook se disculpa por clasificar a los negros como primates</h1>
<p>Este es un artículo auténtico escrito por <a href="https://matrix.to/#/@bramvdnheuvel:nltrix.net">BramvdnHeuvel</a>.</p>
<p>Tiempo estimado de lectura: <img class="icon" src="https://digital-justice.com/images/clock.svg"/> 2 minutos.</p>
</article>
</section>
<section>
<article>
<div>
<h1>Facebook se disculpa por clasificar a los negros como primates</h1>
<p>Las inteligencias artificiales pueden tener sesgos raciales. <a href="https://www.nytimes.com/2019/12/19/technology/facial-recognition-bias.html" target="_blank">Los rostros caucásicos tienen entre 10 y 100 veces más probabilidades de ser reconocidos e identificados correctamente</a> que los rostros afroamericanos y asiáticos, lo que ya ha dado lugar a <a href="https://www.nytimes.com/2020/06/24/technology/facial-recognition-arrest.html" target="_blank">acusaciones erróneas, detenciones e incluso encarcelamientos</a> porque una inteligencia artificial no pudo acertar con un rostro no blanco.</p>
<p>Muchos estudiantes lo han vivido en primera persona durante la pandemia. Muchas empresas como <a href="https://proctorio.com/" target="_blank">Proctorio</a> han cosechado grandes éxitos en universidades y escuelas, donde los profesores instruyen a sus alumnos para que instalen un software que comprueba si están haciendo un examen honestamente. El software era más propenso a culpar a las personas de color de hacer trampas porque <a href="https://micky.com.au/proctorio-test-software-fails-to-detect-people-of-color/" target="_blank">el software no podía reconocer sus caras</a>. Se ha demostrado que este tipo de software de verificación <a href="https://www.technologyreview.com/2020/08/07/1006132/software-algorithms-proctoring-online-tests-ai-ethics/" target="_blank">refuerza la supremacía blanca, el sexismo, el capacitismo y la transfobia</a>.</p>
<h2>El error de Facebook</h2>
<p>La disculpa de Facebook se refería a hombres negros en altercados con civiles blancos y policías, <a href="https://www.nytimes.com/2021/09/03/technology/facebook-ai-race-primates.html" target="_blank">según The New York Times</a>. Una IA de reconocimiento de imágenes clasificó los clips como imágenes de monos o primates, aunque los vídeos no tenían nada que ver con ninguno de ellos.</p>
<p>¿Cómo ocurre esto? Un sesgo en una inteligencia artificial suele ser el resultado de un conjunto de entrenamiento sesgado. Como explica Joy Buolamwini en <a href="https://www.ted.com/talks/joy_buolamwini_how_i_m_fighting_bias_in_algorithms#t-257110" target="_blank">su charla TED</a>, donde explica cómo el software de reconocimiento facial no pudo reconocer su cara. Del mismo modo, un conjunto de datos <a href="https://www.nytimes.com/2018/02/09/technology/facial-recognition-race-artificial-intelligence.html" target="_blank">en el que más del 80 por ciento de los rostros es blanco</a> puede tener más dificultades para reconocer a personas de diferentes colores de piel y puede recurrir a clasificar esos rostros como algo parecido a un humano, como los monos y otros primates.</p>
<p>Un escándalo similar al reciente error de Facebook se vio en 2015, cuando Google <a href="https://eu.usatoday.com/story/tech/2015/07/01/google-apologizes-after-photos-identify-black-people-as-gorillas/29567465/">identificó por error a personas de raza negra como gorilas en Google Photos</a>. Sin embargo, en lugar de cambiar la inteligencia artificial, <a href="https://www.wired.com/story/when-it-comes-to-gorillas-google-photos-remains-blind/" target="_blank">Google Photos se deshizo de palabras como "gorila", "chimpancé", "chimpancé" y "mono". Aunque esto significa que los humanos ya no serán identificados como monos en Google Photos, sugiere que los problemas subyacentes de la IA aún no han sido solucionados.</a></p>
<h2>La mala conducta racial de Facebook</h2>
<p>Aunque el error de la IA de Facebook proviene de un problema subyacente que juega a nivel social, Facebook tiene un historial de abusos, a menudo raciales.</p>
<ul>
<li>Los usuarios de Instagram cuya actividad sugería que eran negros tenían un 50% más de probabilidades de que sus cuentas fueran desactivadas automáticamente. <a href="https://www.nbcnews.com/tech/tech-news/facebook-management-ignored-internal-research-showing-racial-bias-current-former-n1234746" target="_blank">Los investigadores recibieron la orden de sus superiores de detener la investigación y guardar silencio al respecto</a>;</li>
<li>Mark Zuckerberg, CEO de Facebook, <a href="https://gizmodo.com/mark-zuckerberg-asks-racist-facebook-employees-to-stop-1761272768" target="_blank">tuvo que pedir públicamente a los empleados que dejaran de tachar los lemas de Black Lives Matter</a>;</li>
<li>Facebook <a href="https://www.nytimes.com/2021/08/11/technology/facebook-soccer-racism.html" target="_blank">fracasó en el intento de frenar los abusos racistas contra los jugadores de fútbol de Inglaterra</a>;</li>
<li>Varias horribles <a href="https://medium.com/@blindfb2020/facebook-empowers-racism-against-its-employees-of-color-fbbfaf55ab76" target="_blank">historias de racismo e intolerancia entre los empleados de Facebook</a> han sido denunciadas de forma anónima.</li>
</ul>
<p>Ejemplos como estos dejan mucho espacio para preguntarse cuán sinceramente se toma Facebook el desajuste de los primates - y para preguntarse si la disculpa es simplemente una acción de relaciones públicas para evitar más controversia.</p>
<h2>¿Qué podemos hacer al respecto?</h2>
<p>El reconocimiento de imágenes es una herramienta muy útil que puede ayudarnos a mejorar nuestra vida cotidiana, pero la innovación no debe venir acompañada de la discriminación o el refuerzo del fanatismo. La clasificación de un humano como simio es de tal indignidad, y no es más que obvio que Facebook como empresa debería ser responsable.</p>
<p>Es trivial que un simple algoritmo pueda tener consecuencias importantes en una gran plataforma, y por ello los algoritmos deberían ser tratados de esa manera. Un algoritmo de reconocimiento de rostros no debería ser algo que se pueda lanzar a millones de personas, y clasificar a miles de personas en la sección de gorilas es tan inaceptable como denunciar públicamente a esas personas como monos.</p>
<p>Que no baste con una disculpa, sobre todo para una empresa con semejante historial de abusos raciales.</p>
</div>
</article>
</section>
</main>
<footer>
<p>Este sitio web fue fundado por <a href="https://noordstar.me/">BramvdnHeuvel</a>. Aquí tienes <a href="https://github.com/BramvdnHeuvel/Digital-Justice">el código fuente.</a><br/>Puedes contactar con ellos por <a href="mailto:digital-rights@bram.blmgroep.nl">email</a> o en <a href="https://matrix.to/#/#digital-justice:noordstar.me">Matrix</a>.</p>
<script src="https://digital-justice.com/js/expand-iframes.js"></script>
<script src="https://digital-justice.com/js/language-picker.js"></script>
</footer>
</body>
</html>
