<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <!-- Open Sans font -->
    <link id="u-theme-google-font" rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:200,200i,300,300i,400,400i,600,600i,700,700i,900,900i|Open+Sans:300,300i,400,400i,600,600i,700,700i,800,800i">
    <!-- Playfair Display font-->
    <link id="u-page-google-font" rel="stylesheet" href="https://fonts.googleapis.com/css?family=Playfair+Display:400,400i,500,500i,600,600i,700,700i,800,800i,900,900i">

    <!-- CSS -->
    <link rel="stylesheet" type="text/css" href="https://digital-justice.com/all.css">
    <link rel="stylesheet" media="screen and (max-width: 48em)" href="https://digital-justice.com/small.css">

    <!-- Meta tags -->
    <!-- Primary Meta Tags -->
    <title>Facebook apologises for classifying black men as primates</title>
    <meta name="title" content="Facebook apologises for classifying black men as primates">
    <meta name="description" content="One can never guarantee that an AI delivers some awful side-effects, but Facebook's history of racial abuse makes it likely that this mistake won't be their last.">

    <!-- FontAwesome -->
    <link rel="stylesheet" type="text/css" href="https://digital-justice.com/icons/fontawesome-free-5.13.0-web/css/all.css">

    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://digital-justice.com/">
    <meta property="og:title" content="Facebook apologises for classifying black men as primates">
    <meta property="og:description" content="One can never guarantee that an AI delivers some awful side-effects, but Facebook's history of racial abuse makes it likely that this mistake won't be their last.">
    <meta property="og:image" content="https://digital-justice.com/images/data-sovereignty-nologo.png">

    <!-- Twitter -->
    <meta property="twitter:card" content="summary_large_image">
    <meta property="twitter:url" content="https://digital-justice.com/">
    <meta property="twitter:title" content="Facebook apologises for classifying black men as primates">
    <meta property="twitter:description" content="One can never guarantee that an AI delivers some awful side-effects, but Facebook's history of racial abuse makes it likely that this mistake won't be their last.">
    <meta property="twitter:image" content="https://digital-justice.com/images/data-sovereignty-nologo.png">
    
    <!-- Favicon -->
    <!-- Favicon -->
    <link rel="apple-touch-icon" sizes="180x180" href="https://digital-justice.com/icons/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="https://digital-justice.com/icons/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="https://digital-justice.com/icons/favicon-16x16.png">
    <link rel="manifest" href="https://digital-justice.com/icons/site.webmanifest">

</head>
<body>
    <header>
        <nav>
            <a href="../index.html"><i class="fas fa-home"></i> Home</a>

            <a href="../rights.html">Digital rights</a>
            <a href="index.html">Articles</a>
            
            <div id="language-picker"></div>
            <a href="javascript:void(0)" class="hamburger" onclick="openHamburger()">
                <i class="fa fa-bars"></i>
            </a>
        </nav>
        <script src="https://digital-justice.com/js/open-hamburger.js"></script>
    </header>
    <main>
        <section>
            <article>
                <h1>Facebook apologises for classifying black men as primates</h1>
                <p>
                    This is an authentic article written by <a href="https://matrix.to/#/@bramvdnheuvel:nltrix.net">BramvdnHeuvel</a>.
                </p>
                <p>
                    Estimated reading time: <img src="https://digital-justice.com/images/clock.svg" class="icon"> 2 mins.
                </p>
            </article>
        </section>
        <section>
            <article>
                <div>
                    <h1>Facebook apologises for classifying black men as primates</h1>
                    <p>
                        Artifical intelligences can have racial biases. <a href="https://www.nytimes.com/2019/12/19/technology/facial-recognition-bias.html" target="_blank">Caucasian faces are 10 to 100 times more likely to be correctly recognized and identified</a> than African-American faces and Asian faces, which has already led to <a href="https://www.nytimes.com/2020/06/24/technology/facial-recognition-arrest.html" target="_blank">wrongful accusations, arrestations and even imprsionments</a> because an artificial intelligence couldn't correctly match a non-white face.
                    </p>
                    <p>
                        Many students have experienced this first-hand during the pandemic. Many companies like <a href="https://proctorio.com/" target="_blank">Proctorio</a> have made large successes at universities and schools, where teachers instruct their students to install software that checks whether they're making a test honestly. The software was more likely to blame people of color of cheating because <a href="https://micky.com.au/proctorio-test-software-fails-to-detect-people-of-color/" target="_blank">the software couldn't recognize their faces</a>. Such proctoring software has shown to <a href="https://www.technologyreview.com/2020/08/07/1006132/software-algorithms-proctoring-online-tests-ai-ethics/" target="_blank">reinforce white supremacy, sexism, ableism and transphobia</a>.
                    </p>
                    <h2>Facebook's mistake</h2>
                    <p>
                        Facebook's apology concerned black men in altercations with white civilians and police officers, <a href="https://www.nytimes.com/2021/09/03/technology/facebook-ai-race-primates.html" target="_blank">according to The New York Times</a>. An image recognition AI classified the clips as footage of monkeys or primates, even though the videos had nothing to do with either.
                    </p>
                    <p>
                        How does this happen? A bias in an artificial intelligence is usually a result of a biased training set. As Joy Buolamwini explains in <a href="https://www.ted.com/talks/joy_buolamwini_how_i_m_fighting_bias_in_algorithms#t-257110" target="_blank">her TED talk</a>, where she explains how face recognition software couldn't recognize her face. The same way, a data set <a href="https://www.nytimes.com/2018/02/09/technology/facial-recognition-race-artificial-intelligence.html" target="_blank">where over 80 percent of the faces is white</a> may have a harder time recognizing people of different skin colors and may resort to classifying that faces as something close to a human - like monkeys and other primates. 
                    </p>
                    <p>
                        A similar scandal to Facebook's recent mistake was seen in 2015, when Google <a href="https://eu.usatoday.com/story/tech/2015/07/01/google-apologizes-after-photos-identify-black-people-as-gorillas/29567465/">mistakenly identified black people as gorillas in Google Photos</a>. However, instead of changing the artificial intelligence, <a href="https://www.wired.com/story/when-it-comes-to-gorillas-google-photos-remains-blind/" target="_blank">Google Photos got rid of words like "gorilla", "chimp", "chimpanzee" and "monkey". Although this means that humans will no longer be identified as monkeys on Google Photos, it suggests that the underlying problems with the AI haven't been fixed yet.</a>
                    </p>
                    <h2>Facebook's racial misconduct</h2>
                    <p>
                        Although the Facebook AI's mistake comes from an underlying issue that plays on a societal level, Facebook has a record of abuse, often racial.
                    </p>
                    <ul>
                        <li>
                            Instagram users whose activity suggested they were black were 50% more likely to have their accounts automatically disabled. <a href="https://www.nbcnews.com/tech/tech-news/facebook-management-ignored-internal-research-showing-racial-bias-current-former-n1234746" target="_blank">The researchers were told by their superiors to stop the research and to stay silent about it</a>;
                        </li>
                        <li>
                            Mark Zuckerberg, Facebook's CEO, <a href="https://gizmodo.com/mark-zuckerberg-asks-racist-facebook-employees-to-stop-1761272768" target="_blank">had to publicly ask employees to stop crossing out Black Lives Matter slogans</a>;
                        </li>
                        <li>
                            Facebook <a href="https://www.nytimes.com/2021/08/11/technology/facebook-soccer-racism.html" target="_blank">failed to stem racist abuse of England's soccer players</a>;
                        </li>
                        <li>
                            Several horrifying <a href="https://medium.com/@blindfb2020/facebook-empowers-racism-against-its-employees-of-color-fbbfaf55ab76" target="_blank">stories of racism and bigotry among Facebook employees</a> have been reported anonymously.
                        </li>
                    </ul>
                    <p>
                        Examples like these leave plenty of room to wonder how sincerely Facebook takes the primate mismatch -- and to wonder whether the apoligy is simply a PR action to prevent further controversy.
                    </p>
                    <h2>What can we do about it?</h2>
                    <p>
                        Image recognition is a very useful tool that can help us improve our daily lives, but innovation must not come with discrimination or the reinforcement of bigotry. The classification of a human as an ape is of such indignity, and it's nothing but obvious that Facebook as a company should be held responsible.
                    </p>
                    <p>
                        It is only trivial that a simple algorithm can lead to major consequences on a large platform, and hence should algorithms be treated that way. A face recognition algorithm shouldn't simply be something you can just throw onto millions of people, and categorizing thousands of people into the gorilla section is as unacceptable as publicly denouncing those people as monkeys.
                    </p>
                    <p>
                        Don't let an apology be enough, especially for a company with such a track record of racial abuse.
                    </p>
                </div>
            </article>
        </section>
    </main>
    <footer>
        <p>
            This website was founded by <a href="https://noordstar.me/">BramvdnHeuvel</a>. Here's <a href="https://github.com/BramvdnHeuvel/Digital-Justice">the source code.</a><br>
            You can contact them per <a href="mailto:digital-rights@bram.blmgroep.nl">email</a> or on <a href="https://matrix.to/#/#digital-justice:noordstar.me">Matrix</a>.
        </p>
        <script src="https://digital-justice.com/js/expand-iframes.js"></script>
        <script src="https://digital-justice.com/js/language-picker.js"></script>
    </footer>
</body>
</html>
