<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<!-- Open Sans font -->
<link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:200,200i,300,300i,400,400i,600,600i,700,700i,900,900i|Open+Sans:300,300i,400,400i,600,600i,700,700i,800,800i" id="u-theme-google-font" rel="stylesheet"/>
<!-- Playfair Display font-->
<link href="https://fonts.googleapis.com/css?family=Playfair+Display:400,400i,500,500i,600,600i,700,700i,800,800i,900,900i" id="u-page-google-font" rel="stylesheet"/>
<!-- CSS -->
<link href="https://digital-justice.com/all.css" rel="stylesheet" type="text/css"/>
<link href="https://digital-justice.com/small.css" media="screen and (max-width: 48em)" rel="stylesheet"/>
<!-- Meta tags -->
<!-- Primary Meta Tags -->
<title>O Facebook pede desculpa por classificar os homens negros como primatas</title>
<meta content="O Facebook pede desculpa por classificar os homens negros como primatas" name="title"/>
<meta content="Nunca se pode garantir que uma IA produza alguns efeitos secundários terríveis, mas a história de abuso racial do Facebook torna provável que este erro não seja o seu último." name="description"/>
<!-- FontAwesome -->
<link href="https://digital-justice.com/icons/fontawesome-free-5.13.0-web/css/all.css" rel="stylesheet" type="text/css"/>
<!-- Open Graph / Facebook -->
<meta content="website" property="og:type"/>
<meta content="https://digital-justice.com/" property="og:url"/>
<meta content="O Facebook pede desculpa por classificar os homens negros como primatas" property="og:title"/>
<meta content="Nunca se pode garantir que uma IA produza alguns efeitos secundários terríveis, mas a história de abuso racial do Facebook torna provável que este erro não seja o seu último." property="og:description"/>
<meta content="https://digital-justice.com/images/data-sovereignty-nologo.png" property="og:image"/>
<!-- Twitter -->
<meta content="summary_large_image" property="twitter:card"/>
<meta content="https://digital-justice.com/" property="twitter:url"/>
<meta content="O Facebook pede desculpa por classificar os homens negros como primatas" property="twitter:title"/>
<meta content="Nunca se pode garantir que uma IA produza alguns efeitos secundários terríveis, mas a história de abuso racial do Facebook torna provável que este erro não seja o seu último." property="twitter:description"/>
<meta content="https://digital-justice.com/images/data-sovereignty-nologo.png" property="twitter:image"/>
<!-- Favicon -->
<!-- Favicon -->
<link href="https://digital-justice.com/icons/apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180"/>
<link href="https://digital-justice.com/icons/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png"/>
<link href="https://digital-justice.com/icons/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png"/>
<link href="https://digital-justice.com/icons/site.webmanifest" rel="manifest"/>
</head>
<body>
<header>
<nav>
<a href="../index.html"><i class="fas fa-home"></i> Home</a>
<a href="../rights.html">Direitos digitais</a>
<a href="index.html">Artigos</a>
<div id="language-picker"></div>
<a class="hamburger" href="javascript:void(0)" onclick="openHamburger()">
<i class="fa fa-bars"></i>
</a>
</nav>
<script src="https://digital-justice.com/js/open-hamburger.js"></script>
</header>
<main>
<section>
<article>
<h1>O Facebook pede desculpa por classificar os homens negros como primatas</h1>
<p>Este é um artigo autêntico escrito por <a href="_COPY19@bramvdnheuvel:nltrix.net">BramvdnHeuvel</a>.</p>
<p>Tempo estimado de leitura: <img class="icon" src="https://digital-justice.com/images/clock.svg"/> 2 mins.</p>
</article>
</section>
<section>
<article>
<div>
<h1>O Facebook pede desculpa por classificar os homens negros como primatas</h1>
<p>As inteligências artificiais podem ter preconceitos raciais. <a href="https://www.nytimes.com/2019/12/19/technology/facial-recognition-bias.html" target="_blank">Rostos caucasianos têm 10 a 100 vezes mais probabilidades de serem correctamente reconhecidos e identificados</a> do que rostos afro-americanos e asiáticos, o que já levou a <a href="https://www.nytimes.com/2020/06/24/technology/facial-recognition-arrest.html" target="_blank">acusações erradas, detenções e até impressões</a> porque uma inteligência artificial não conseguia corresponder correctamente a um rosto não branco.</p>
<p>Muitos estudantes já experimentaram isto em primeira mão durante a pandemia. Muitas empresas como <a href="https://proctorio.com/" target="_blank">Proctorio</a> fizeram grandes sucessos em universidades e escolas, onde os professores instruem os seus alunos a instalar software que verifica se estão a fazer um teste de forma honesta. O software tinha mais probabilidades de culpar pessoas de cor de batota porque <a href="https://micky.com.au/proctorio-test-software-fails-to-detect-people-of-color/" target="_blank">o software não conseguia reconhecer os seus rostos</a>. Este software de sondagem mostrou a <a href="https://www.technologyreview.com/2020/08/07/1006132/software-algorithms-proctoring-online-tests-ai-ethics/" target="_blank">reforce a supremacia branca, o sexismo, a capacidade e a transfobia</a>.</p>
<h2>Erro do Facebook</h2>
<p>As desculpas do Facebook diziam respeito aos homens negros em altercações com civis brancos e agentes da polícia, <a href="https://www.nytimes.com/2021/09/03/technology/facebook-ai-race-primates.html" target="_blank">de acordo com o The New York Times</a>. Uma IA de reconhecimento de imagem classificou os clips como filmagens de macacos ou primatas, embora os vídeos não tivessem nada a ver com nenhum dos dois.</p>
<p>Como é que isto acontece? Um enviesamento numa inteligência artificial é geralmente o resultado de um conjunto de treino enviesado. Como Joy Buolamwini explica em <a href="https://www.ted.com/talks/joy_buolamwini_how_i_m_fighting_bias_in_algorithms#t-257110" target="_blank">her TED talk</a>, onde ela explica como o software de reconhecimento facial não conseguia reconhecer o seu rosto. Da mesma forma, um conjunto de dados <a href="https://www.nytimes.com/2018/02/09/technology/facial-recognition-race-artificial-intelligence.html" target="_blank">onde mais de 80 por cento dos rostos são brancos</a> pode ter mais dificuldade em reconhecer pessoas de diferentes cores de pele e pode recorrer à classificação desses rostos como algo próximo de um humano - como macacos e outros primatas.</p>
<p>Um escândalo semelhante ao recente erro do Facebook foi visto em 2015, quando o Google <a href="https://eu.usatoday.com/story/tech/2015/07/01/google-apologizes-after-photos-identify-black-people-as-gorillas/29567465/">mistakenly identificou os negros como gorilas no Google Photos</a>. Contudo, em vez de mudar a inteligência artificial, <a href="https://www.wired.com/story/when-it-comes-to-gorillas-google-photos-remains-blind/" target="_blank">Google Photos livrou-se de palavras como "gorila", "chimpanzé", "chimpanzé" e "macaco". Embora isto signifique que os humanos já não serão identificados como macacos no Google Photos, sugere que os problemas subjacentes com a IA ainda não foram resolvidos.</a></p>
<h2>A má conduta racial do Facebook</h2>
<p>Embora o erro da IA do Facebook venha de uma questão subjacente que toca a nível social, o Facebook tem um historial de abusos, muitas vezes raciais.</p>
<ul>
<li>Os utilizadores do Instagram cuja actividade sugeria que eram negros tinham 50% mais probabilidades de ter as suas contas automaticamente desactivadas. <a href="https://www.nbcnews.com/tech/tech-news/facebook-management-ignored-internal-research-showing-racial-bias-current-former-n1234746" target="_blank">Os investigadores foram aconselhados pelos seus superiores hierárquicos a parar a investigação e a permanecer em silêncio sobre a mesma</a>;</li>
<li>Mark Zuckerberg, Director Executivo do Facebook, <a href="https://gizmodo.com/mark-zuckerberg-asks-racist-facebook-employees-to-stop-1761272768" target="_blank">deveria pedir publicamente aos funcionários que parassem de riscar os slogans de Black Lives Matter</a>;</li>
<li>Facebook <a href="https://www.nytimes.com/2021/08/11/technology/facebook-soccer-racism.html" target="_blank">failed to stem racist abuse of England's soccer players</a>;</li>
<li>Várias <a href="https://medium.com/@blindfb2020/facebook-empowers-racism-against-its-employees-of-color-fbbfaf55ab76" target="_blank">stórias de racismo e fanatismo entre funcionários do Facebook</a> têm sido relatadas anonimamente.</li>
</ul>
<p>Exemplos como estes deixam muito espaço para nos interrogarmos sobre a sinceridade com que o Facebook toma o desajustamento dos primatas - e para nos perguntarmos se a apoligia é simplesmente uma acção de relações públicas para evitar mais controvérsia.</p>
<h2>O que podemos fazer quanto a isso?</h2>
<p>O reconhecimento da imagem é um instrumento muito útil que nos pode ajudar a melhorar a nossa vida diária, mas a inovação não deve vir com a discriminação ou o reforço do fanatismo. A classificação de um humano como macaco é de tal indignidade, e não é mais do que óbvio que o Facebook como empresa deve ser considerado responsável.</p>
<p>É apenas trivial que um algoritmo simples possa levar a grandes consequências numa grande plataforma, e por isso os algoritmos devem ser tratados dessa forma. Um algoritmo de reconhecimento facial não deve ser simplesmente algo que se pode atirar a milhões de pessoas, e categorizar milhares de pessoas na secção gorila é tão inaceitável como denunciar publicamente essas pessoas como macacos.</p>
<p>Não deixe que um pedido de desculpas seja suficiente, especialmente para uma empresa com um tal historial de abuso racial.</p>
</div>
</article>
</section>
</main>
<footer>
<p>Este sítio foi fundado por <a href="https://noordstar.me/">BramvdnHeuvel</a>. Aqui está <a href="https://github.com/BramvdnHeuvel/Digital-Justice">o código fonte.</a><br/> Pode contactá-los por <a href="mailto:digital-rights@bram.blmgroep.nl">email</a> ou em <a href="https://matrix.to/#/#digital-justice:noordstar.me">Matrix</a>.</p>
<script src="https://digital-justice.com/js/expand-iframes.js"></script>
<script src="https://digital-justice.com/js/language-picker.js"></script>
</footer>
</body>
</html>
