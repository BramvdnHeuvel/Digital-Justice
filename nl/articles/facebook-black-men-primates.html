<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<!-- Open Sans font -->
<link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:200,200i,300,300i,400,400i,600,600i,700,700i,900,900i|Open+Sans:300,300i,400,400i,600,600i,700,700i,800,800i" id="u-theme-google-font" rel="stylesheet"/>
<!-- Playfair Display font-->
<link href="https://fonts.googleapis.com/css?family=Playfair+Display:400,400i,500,500i,600,600i,700,700i,800,800i,900,900i" id="u-page-google-font" rel="stylesheet"/>
<!-- CSS -->
<link href="https://digital-justice.com/all.css" rel="stylesheet" type="text/css"/>
<!-- Meta tags -->
<!-- Primary Meta Tags -->
<title>Facebook verontschuldigt zich voor het classificeren van zwarte mannen als primaten</title>
<meta content="Facebook verontschuldigt zich voor het classificeren van zwarte mannen als primaten" name="title"/>
<meta content="Men kan nooit garanderen dat een AI een aantal vreselijke bijwerkingen oplevert, maar Facebooks geschiedenis van racistisch misbruik maakt het waarschijnlijk dat deze fout niet hun laatste zal zijn." name="description"/>
<!-- FontAwesome -->
<link href="https://digital-justice.com/icons/fontawesome-free-5.13.0-web/css/all.css" rel="stylesheet" type="text/css"/>
<!-- Open Graph / Facebook -->
<meta content="website" property="og:type"/>
<meta content="https://digital-justice.com/" property="og:url"/>
<meta content="Facebook verontschuldigt zich voor het classificeren van zwarte mannen als primaten" property="og:title"/>
<meta content="Men kan nooit garanderen dat een AI een aantal vreselijke bijwerkingen oplevert, maar Facebooks geschiedenis van racistisch misbruik maakt het waarschijnlijk dat deze fout niet hun laatste zal zijn." property="og:description"/>
<meta content="https://digital-justice.com/images/data-sovereignty-nologo.png" property="og:image"/>
<!-- Twitter -->
<meta content="summary_large_image" property="twitter:card"/>
<meta content="https://digital-justice.com/" property="twitter:url"/>
<meta content="Facebook verontschuldigt zich voor het classificeren van zwarte mannen als primaten" property="twitter:title"/>
<meta content="Men kan nooit garanderen dat een AI een aantal vreselijke bijwerkingen oplevert, maar Facebooks geschiedenis van racistisch misbruik maakt het waarschijnlijk dat deze fout niet hun laatste zal zijn." property="twitter:description"/>
<meta content="https://digital-justice.com/images/data-sovereignty-nologo.png" property="twitter:image"/>
<!-- Favicon -->
<!-- Favicon -->
<link href="https://digital-justice.com/icons/apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180"/>
<link href="https://digital-justice.com/icons/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png"/>
<link href="https://digital-justice.com/icons/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png"/>
<link href="https://digital-justice.com/icons/site.webmanifest" rel="manifest"/>
</head>
<body>
<header>
<nav>
<a href="../index.html"><i class="fas fa-home"></i> Home</a>
<a href="../rights.html">Digitale rechten</a>
<a href="index.html">Artikelen</a>
<div id="language-picker"></div>
<a class="hamburger" href="javascript:void(0)" onclick="openHamburger()">
<i class="fa fa-bars"></i>
</a>
</nav>
<script src="https://digital-justice.com/js/open-hamburger.js"></script>
</header>
<section class="simple-orange">
<article class="essay-title">
<h1>Facebook verontschuldigt zich voor het classificeren van zwarte mannen als primaten</h1>
<p>Dit is een authentiek artikel geschreven door <a href="https://matrix.to/#/@bramvdnheuvel:nltrix.net">BramvdnHeuvel</a>.</p>
<p>Geschatte leestijd: <img class="icon" src="https://digital-justice.com/images/clock.svg"/> 2 minuten.</p>
</article>
</section>
<section class="simple-gray flex-container">
<article class="blog">
<h1>Facebook verontschuldigt zich voor het classificeren van zwarte mannen als primaten</h1>
<p>Kunstmatige intelligenties kunnen raciale vooroordelen hebben. <a href="https://www.nytimes.com/2019/12/19/technology/facial-recognition-bias.html" target="_blank">Kaukasische gezichten hebben 10 tot 100 keer meer kans om correct herkend en geïdentificeerd te worden</a> dan Afro-Amerikaanse gezichten en Aziatische gezichten, wat al heeft geleid tot <a href="https://www.nytimes.com/2020/06/24/technology/facial-recognition-arrest.html" target="_blank">onrechtmatige beschuldigingen, arrestaties en zelfs opsluitingen</a> omdat een kunstmatige intelligentie een niet-blank gezicht niet correct kon matchen.</p>
<p>Veel studenten hebben dit tijdens de pandemie aan den lijve ondervonden. Veel bedrijven zoals <a href="https://proctorio.com/" target="_blank">Proctorio</a> hebben grote successen geboekt op universiteiten en scholen, waar docenten hun studenten instrueren software te installeren die controleert of ze een toets eerlijk maken. De software gaf gekleurde mensen eerder de schuld van spieken omdat <a href="https://micky.com.au/proctorio-test-software-fails-to-detect-people-of-color/" target="_blank">de software hun gezichten niet kon herkennen</a>. Dergelijke proctoring software heeft aangetoond <a href="https://www.technologyreview.com/2020/08/07/1006132/software-algorithms-proctoring-online-tests-ai-ethics/" target="_blank">blanke suprematie, seksisme, ableisme en transfobie</a> te versterken.</p>
<h2>Facebook's fout</h2>
<p>Facebook's verontschuldiging betrof zwarte mannen in woordenwisselingen met blanke burgers en politieagenten, <a href="https://www.nytimes.com/2021/09/03/technology/facebook-ai-race-primates.html" target="_blank">volgens The New York Times</a>. Een beeldherkennings AI classificeerde de clips als beelden van apen of primaten, ook al hadden de video's niets te maken met een van beide.</p>
<p>Hoe gebeurt dit? Een vooroordeel in een kunstmatige intelligentie is meestal het gevolg van een bevooroordeelde trainingsset. Zoals Joy Buolamwini uitlegt in <a href="https://www.ted.com/talks/joy_buolamwini_how_i_m_fighting_bias_in_algorithms#t-257110" target="_blank">haar TED talk</a>, waarin ze uitlegt hoe gezichtsherkenningssoftware haar gezicht niet kon herkennen. Op dezelfde manier kan een dataset <a href="https://www.nytimes.com/2018/02/09/technology/facial-recognition-race-artificial-intelligence.html" target="_blank">waarin meer dan 80 procent van de gezichten blank is</a> het moeilijker hebben om mensen met een andere huidskleur te herkennen en kan die overgaan tot het classificeren van die gezichten als iets dat dicht bij een mens staat - zoals apen en andere primaten.</p>
<p>Een soortgelijk schandaal als Facebook's recente fout was te zien in 2015, toen Google <a href="https://eu.usatoday.com/story/tech/2015/07/01/google-apologizes-after-photos-identify-black-people-as-gorillas/29567465/">per abuis zwarte mensen identificeerde als gorilla's in Google Photos</a>. Echter, in plaats van de kunstmatige intelligentie te veranderen, <a href="https://www.wired.com/story/when-it-comes-to-gorillas-google-photos-remains-blind/" target="_blank">schafte Google Photos woorden als "gorilla", "chimpansee" en "aap" af. Hoewel dit betekent dat mensen niet langer zullen worden geïdentificeerd als apen op Google Photos, suggereert het dat de onderliggende problemen met de AI nog niet zijn opgelost.</a></p>
<h2>Facebook's racistische wangedrag</h2>
<p>Hoewel de fout van de Facebook AI voortkomt uit een onderliggende kwestie die speelt op een maatschappelijk niveau, heeft Facebook een record van misbruik, vaak racistisch.</p>
<ul>
<li>Instagram-gebruikers wier activiteit suggereerde dat ze zwart waren, hadden 50% meer kans dat hun accounts automatisch werden uitgeschakeld. <a href="https://www.nbcnews.com/tech/tech-news/facebook-management-ignored-internal-research-showing-racial-bias-current-former-n1234746" target="_blank">De onderzoekers kregen van hun superieuren te horen dat ze moesten stoppen met het onderzoek en erover moesten zwijgen</a>;</li>
<li>Mark Zuckerberg, CEO van Facebook, <a href="https://gizmodo.com/mark-zuckerberg-asks-racist-facebook-employees-to-stop-1761272768" target="_blank">had publiekelijk aan medewerkers moeten vragen te stoppen met het doorstrepen van Black Lives Matter slogans</a>;</li>
<li>Facebook <a href="https://www.nytimes.com/2021/08/11/technology/facebook-soccer-racism.html" target="_blank">faalde in het tegengaan van racistisch misbruik van Engelse voetballers</a>;</li>
<li>Verschillende gruwelijke <a href="https://medium.com/@blindfb2020/facebook-empowers-racism-against-its-employees-of-color-fbbfaf55ab76" target="_blank">verhalen over racisme en onverdraagzaamheid onder Facebook medewerkers</a> zijn anoniem gemeld.</li>
</ul>
<p>Voorbeelden als deze laten genoeg ruimte om je af te vragen hoe oprecht Facebook de primatenmismatch neemt -- en om je af te vragen of de apoligy niet gewoon een PR-actie is om verdere controverse te voorkomen.</p>
<h2>Wat kunnen we er aan doen?</h2>
<p>Beeldherkenning is een zeer nuttig instrument dat ons kan helpen ons dagelijks leven te verbeteren, maar innovatie mag niet gepaard gaan met discriminatie of de versterking van onverdraagzaamheid. De classificatie van een mens als een aap is van een dergelijke vernedering, en het is niets anders dan duidelijk dat Facebook als bedrijf verantwoordelijk moet worden gehouden.</p>
<p>Het is alleen triviaal dat een simpel algoritme tot grote gevolgen kan leiden op een groot platform, en dus moeten algoritmes ook zo behandeld worden. Een gezichtsherkenningsalgoritme mag niet zomaar op miljoenen mensen gegooid worden, en duizenden mensen in de gorilla-afdeling indelen is even onaanvaardbaar als die mensen publiekelijk als apen aan te klagen.</p>
<p>Laat een verontschuldiging niet genoeg zijn, zeker niet voor een bedrijf met zo'n staat van dienst op het gebied van racistisch wangedrag.</p>
</article>
</section>
<footer>
<p>Deze website is gebouwd door <a href="https://noordstar.me/">BramvdnHeuvel</a>.<br/> U kunt contact met hen opnemen via <a href="mailto:digital-rights@bram.blmgroep.nl">email</a> of via <a href="https://matrix.to/#/@bramvdnheuvel:nltrix.net">Matrix</a>.</p>
<script src="https://digital-justice.com/js/expand-iframes.js"></script>
<script src="https://digital-justice.com/js/language-picker.js"></script>
</footer>
</body>
</html>
